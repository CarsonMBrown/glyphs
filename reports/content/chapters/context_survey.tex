\section{Binarization}
As many of the most common, commercial, and competitive optical character recognition (OCR) models and methods require or work better with a binarized image \cite{Gupta, SmithTesseract, SmithLines, Bar-Yosef2005, Bar-Yosef2007} (also called a bi-level image), it is important to explore binarization as a method to improve the results of the pipeline.
Various methods are utilized for binarization of historical manuscripts, including thresholding \cite{Bar-Yosef2005, Bar-Yosef2007}, pixel clustering \cite{Bera}, and convolutional neural networks (CNNs) \cite{Dhali2019, Dhali2020, Xiong}. In each of these methods, the goal is the same, to separate the original markings (ink) on the document from the rest of the image. This usually involves generating a second image where the ink is marked by a black pixel, with the rest of the image being white.

In the case of papyri, this is not a trivial task, requiring techniques to have both high precision and recall to generate useful bi-level images to pass forward to further steps of the OCR pipeline. The strongest methods for binarization are able to not only differentiate ink from papyrus, but also identify extraneous text markings such as ruler marks and annotations added by historians, frames or cases containing the papyri, as well as elements in the image that are either edited in after the image is taken.

Bar-Yosef \cite{Bar-Yosef2005} and Bar-Yosef et al.\cite{Bar-Yosef2005} both utilize a simple thresholding algorithm to generate an initial binarized image to use for further classification.

Bera et al.\cite{Bera} propose an approach to binarization that involves using the fuzzy C-means, K-medoids and K-means++ clustering algorithms to form clusters of pixels into groups representing foreground and background elements. This method also makes use of normalization, to remove noise and shadow, and thresholding, maximizing inter-cluster distance and minimizing intra-cluster distance, thus generating very specific clusters. Pixels are labeled by the clusters and then the final output is determined by a decision algorithm that takes in the labels from the three clustering algorithms. This approach generates an F-measure result of $76.84$ and a peak signal-to-noise ratio of $15.31$, both measured against the 2018 DIBCO (Document Image Binarization COntest) dataset\cite{DIBCO2018}.

Dhali et al.\cite{Dhali2019} and Xiong et al.\cite{Xiong} both utilize CNNs for this task using manually annotated data in combination with transfer learning to retrain existing models to generate new networks with F-measure results of $86.7$ and $92.81$, respectively, and peak signal-to-noise ratios of $21.3$ and $17.56$, respectively (all measured against DIBCO'18). While it has a lower F-measure on the whole of the DIBCO'18 dataset, the network proposed by Dhali et al., BiNet, was trained more specifically on degraded manuscripts and is shown to also be able to remove the extraneous elements from the image including ruler marks, machine-printed color-calibrators, and picture frames.

By combining and contrasting these methods against each other, a high quality, bi-level representation can be generated that can be passed to the next step of an OCR pipeline to bound and classify each glyph with potentially more accuracy than would be possible with an unaltered image.

\section{Glyph Grouping / Bounding Boxes}
Precise bounding boxes are key to the success of every classification method, as removing noise from the edges of the glyphs to increase the signal-to-noise ratio of the image being input into the classification algorithm. Similarly, to utilize a language model to help assist in classification, or to generate document level transcriptions, a method for converting a list of bounding boxes into pairs or larger collections of neighboring glyphs is required.

A method proposed by Garz et al.\cite{Garz2012, Garz2011} utilizes points of interest to find the spaces between lines, which requires no binarization, and can be performed on the source image without first generating bounding boxes, which allows this method to generate potential bounding boxes by detecting clusters of points of interest (generated using Difference of Gaussian \cite{Lowe}). These points of interest are then used to generate bounding boxes using Density-Based Spatial Clustering of Applications with Noise (DBSCAN) \cite{Ester}, which in the work proposed by Garz et al.\cite{Garz2012, Garz2011} requires a manual estimation of neighborhood size to create clusters. However, it could be possible to automatically estimate this distance using a more basic bounding-box finding algorithm or the analytical approach suggested by \cite{Daszykowski}. Points of interest are also utilized to generate lines by splitting the image on the lines of lowest energy (farthest from the points of interest), weighting horizonal movement as lower energy than vertical movement.
This method, analyzed on hand-written text from all 60 pages of the \textit{Saint Gall} database, has a line accuracy of $97.97\%$ over 1431 lines of text, some of which contain background noise. This level of accuracy may allow this method to be successfully adpadted to papyrus.

Williams \cite{Williams2014} also proposes a method for line detection based on finding gaps in the vertical coordinates of the glyphs. By finding the potential centers of each glyph and then sorting all the potential centers by their vertical coordinates, significant gaps in the list are used to separate lines. For each line, Williams then takes the line of best fit, using regression, to determine a potential line. A second pass then separates glyphs that are outside of the standard distance by a certain amount. This method appears to work quite well for glyphs where many potential centers are known, such as for crowdsourced data or binarized data with centroids. However, it fails significantly when a line has enough curvature that two lines occupy the same vertical coordinates over the horizontal span of the image.

In contrast to the algorithmic approaches above, Ezra et al.\cite{Ezra} propose a supervised learning approach for transcription alignment and bounding. Using a CNN-RNN trained to classify glyphs on lines, this method was able to generate results with an accuracy of $90.3\%$. They also achieved a mean bounding box overlap of $81.0\%$ using the same model.

Another potential bounding method that could be explored is utilizing binarization to generate blobs for each glyph and bounding each blob. Combined with any of the above methods, this could create tight bounding boxes for use in classification as the bounding boxes could either be kept binarized or converted back to the full color image, depending on the classification method being used.

Other potential sources for line finding are Tesseract\cite{SmithTesseract} and Ocular\cite{Berg-Kirkpatrick}. Tesseract is a popular OCR engine with the ability to perform line finding \cite{SmithLines} using blobs on images that can be skewed, meaning the image quality is not lost due to warping the images the images to allow for more sensitive techniques to be performed.

\section{Classification}
Once the glyphs are bounded, they need to be classified. This can be done utilizing many methods, some of which could be combined to improve the results.
These methods include utilizing transfer learning and image similarity \cite{Vadicamo, Yuan}, CNN based classification \cite{Yousefi, Haliassos, Swindall}, freeman chains \cite{Althobaiti}, probabilistic models \cite{Berg-Kirkpatrick}, and image deformation \cite{Nederhof, Keysers, Tabin, Gass}. While not all of the methods require binarized images to work, these methods may still be improved by precise binarization, as it would remove the need for the model to handle the noise added by the papyrus itself and any degradation of the papyrus.

Bar-Yosef et al.\cite{Bar-Yosef2007} also expand on the work done by Bar-Yosef by eroding characters to generate 'structuring elements' for each glyph, unique elements of glyphs that can be overlaid over the binarized text to identify characters of each glyph class. Using  this method, Bar-Yosef et al. were able to achieve an accuracy of $94.65\%$ on a test set of 1477 aleph characters over 22 Hebrew documents. By combining the more advanced binarization methods proposed by Xiong et al.\cite{Xiong} and Dhali et al.\cite{Dhali2019} it may be possible to generate similar results on the more degraded, ancient Greek documents targeted by this research.

Using image similarity as proposed by Vadicamo \cite{Vadicamo} and Yuan et al. \cite{Yuan} or image distortion proposed by Keysers et al. \cite{Keysers} could also allow for classification based on referencing known characters, which would allow for the pipeline to be expanded to any glyph set with little effort. Both methods rely on finding glyphs from a dataset that are the closest to the found glyph from the text being transcribed. While image distortion uses algorithmic approaches, image similarity uses CNNs to perform feature extraction from the image before classifying the image, which could utilize distance, clustering, or other unsupervised learning techniques such as random forests.
These methods could both be performed on binarized and non-binarized images, but it is likely that they would perform better on high-quality bi-level images than the full color, noisy images. While these methods do not provide any accuracy measures for classifcation using image similarity, Nederhof \cite{Nederhof} reports an accuracy of $91.3\%$ with accuracy increasing to $95.5\%$ if the two most likely classes are considered.

Similar to the above methods, using Freeman chain encoding could allow for found glyphs to be encoded in a way that would allow for finding similar glyphs from a database of known characters, using either a distance measure or unsupervised learning techniques. In the method proposed by Althobaiti et al. \cite{Althobaiti}, binarized characters are converted to lists of numbers by tracing the outline of the glyph and recording the slope at each pixel as a number from one to eight based on the direction of the next pixel in the outline of the glyph. Using this method on handwritten Arabic text, the authors achieved an average accuracy between $92\%$ and $97\%$ depending on the character. Like the methods described previously, this method would also enable the pipeline to be easily translated to another glyph set without needing to retrain the model.

Finally, several approaches to CNN based classifcation are suggested by Yousefi et al.\cite{Yousefi}, Haliassos et al.\cite{Haliassos}, and Swindall et al.\cite{Swindall}. Yousefi et al. suggest the use of LSTM Networks on non-binarized images, achieving accuracies of less than $1\%$ on greyscale images. However, Yousefi et al. worked with images with very low noise, and thus had significantly more well-formed letters than could be expected on papyrus. For results on datasets closer to what could be expected from ancient papyrus, Haliassos et al.\cite{Haliassos} suggest using Support Vector Machines (SVMs) to classify images of glyphs using Histogram of Ordered Gradients (HOG) features and CNNs to classify glyphs on ancient papyrus, achieving accuracies of $88.71\%$ and $95.77\%$ respectively, with Swindall et al.\cite{Swindall} achieving similar CNN-based results on papyrus with an accuracy of $92.73\%$

\section{Croudsourced Transcriptions}
A large portion of the research in ancient Greek manuscript OCR has been done utilizing crowdsourced citizen science, allowing papyrologists and non-papyrologists alike to annotate ancient Greek manuscripts utilizing various pipelines that allow users to detect and/or classify Greek letters.\cite{Williams2014, Williams2015, Tabin, Atanasiu} These methods, while not helpful in generating a fully automated pipeline, are valuable for generating data for both training and testing of fully automated pipelines.

Like Dhali et al.\cite{Dhali2019}, Tabin\cite{Tabin} utilizes a manual facsimile generation process that allows a user to manually annotate pixels in images, which they use to perform OCR on directly, but that could also be utilized in generating more data to train on as Dhali et al.\cite{Dhali2019} proposed.

Another usage for crowdsourced OCR is proposed by Williams \cite{Williams2014}, in which the consensus transcriptions are compared to known Greek texts to determine if the crowdsourced transcription may be a part of a known text. This allows for error correction and filling in the gaps, as well as the potential for creation of more accurate training data, as damaged or partially-missing glyphs could still be classified correctly if they can be matched to known glyphs from other texts.

This method for transcription, while not able to be directly integrated into a fully automated pipeline for OCR, is a potentially valuable source for training and/or testing data, and could be utilized to verify or improve the results of a future pipeline.
