"C:\Users\Carson Brown\.conda\envs\glyphs\python.exe" "C:/Users/Carson Brown/git/glyphs/src/main.py"
Using cache found in C:\Users\Carson Brown/.cache\torch\hub\pytorch_vision_v0.10.0
Image input directory: dataset\glyphs\eval\raw
C:\Users\Carson Brown\.conda\envs\glyphs\lib\site-packages\torchvision\models\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.
  warnings.warn(
transform Compose(
    ColorJitter(brightness=[0.5, 1.5], contrast=[0.5, 1.5], saturation=[0.5, 1.5], hue=[-0.1, 0.1])
    RandomAffine(degrees=[-10.0, 10.0], shear=[-10.0, 10.0])
    RandomResizedCrop(size=(224, 224), scale=(0.9, 1.1), ratio=(0.9, 1.1), interpolation=bilinear)
    RandomAdjustSharpness(sharpness_factor=2,p=0.5)
    RandomAutocontrast(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    AddGaussianNoise(mean=0.0, std=0.003)
)
Input Vector Size: None
C:\Users\Carson Brown\.conda\envs\glyphs\lib\site-packages\torch\nn\modules\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
Image input directory: dataset\glyphs\train\raw
transform Compose(
    Resize(size=224, interpolation=bilinear, max_size=None, antialias=None)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Training set has 5000 instances
EPOCH 0:
  batch 250 loss: 3.1503277378082277
  batch 500 loss: 3.093536793708801
LOSS train 3.093536793708801 valid 3.0409493446350098
PRECISION 0.03552333333333329 RECALL 0.095 FSCORE 0.04514483405483408
EPOCH 1:
  batch 250 loss: 3.0177065410614015
  batch 500 loss: 2.9459084119796755
LOSS train 2.9459084119796755 valid 2.9251084327697754
PRECISION 0.03022095238095233 RECALL 0.0944 FSCORE 0.038292092352092445
EPOCH 2:
  batch 250 loss: 2.9039957475662233
  batch 500 loss: 2.8744742374420165
LOSS train 2.8744742374420165 valid 2.8885607719421387
PRECISION 0.02017142857142857 RECALL 0.096 FSCORE 0.031243535353535516
EPOCH 3:
  batch 250 loss: 2.8774727602005004
  batch 500 loss: 2.8592004957199095
LOSS train 2.8592004957199095 valid 2.875016689300537
PRECISION 0.019560714285714285 RECALL 0.0962 FSCORE 0.03111191919191936
EPOCH 4:
  batch 250 loss: 2.8630840015411376
  batch 500 loss: 2.848728259086609
LOSS train 2.848728259086609 valid 2.864814281463623
PRECISION 0.020135714285714285 RECALL 0.0964 FSCORE 0.03141525252525271
EPOCH 5:
  batch 250 loss: 2.856660891532898
  batch 500 loss: 2.8459705953598022
LOSS train 2.8459705953598022 valid 2.8609538078308105
PRECISION 0.02316428571428571 RECALL 0.0976 FSCORE 0.03353878787878807
EPOCH 6:
  batch 250 loss: 2.844224720954895
  batch 500 loss: 2.8350001583099367
LOSS train 2.8350001583099367 valid 2.8509976863861084
PRECISION 0.04863880952380945 RECALL 0.1112 FSCORE 0.058095526695526804
EPOCH 7:
  batch 250 loss: 2.8350252704620362
  batch 500 loss: 2.8243244314193725
LOSS train 2.8243244314193725 valid 2.8416566848754883
PRECISION 0.0672616666666667 RECALL 0.1296 FSCORE 0.07525731601731608
EPOCH 8:
  batch 250 loss: 2.8178781633377077
  batch 500 loss: 2.8050569190979004
LOSS train 2.8050569190979004 valid 2.8302056789398193
PRECISION 0.0804097619047619 RECALL 0.1398 FSCORE 0.09014873015873014
EPOCH 9:
  batch 250 loss: 2.8015936822891234
  batch 500 loss: 2.797957489013672
LOSS train 2.797957489013672 valid 2.8277807235717773
PRECISION 0.08045190476190481 RECALL 0.136 FSCORE 0.08802238095238084
EPOCH 10:
  batch 250 loss: 2.7753766384124754
  batch 500 loss: 2.7767430238723754
LOSS train 2.7767430238723754 valid 2.8091447353363037
PRECISION 0.09048357142857151 RECALL 0.1526 FSCORE 0.09867142857142853
EPOCH 11:
  batch 250 loss: 2.7528539934158327
  batch 500 loss: 2.7539528093338013
LOSS train 2.7539528093338013 valid 2.799003839492798
PRECISION 0.08610285714285726 RECALL 0.151 FSCORE 0.09418229437229433
EPOCH 12:
  batch 250 loss: 2.7205997047424315
  batch 500 loss: 2.7273663816452025
LOSS train 2.7273663816452025 valid 2.7872636318206787
PRECISION 0.08554547619047623 RECALL 0.1528 FSCORE 0.09587952380952372
EPOCH 13:
  batch 250 loss: 2.695324119567871
  batch 500 loss: 2.700551206588745
LOSS train 2.700551206588745 valid 2.7947518825531006
PRECISION 0.08524547619047637 RECALL 0.1548 FSCORE 0.09459920634920638
EPOCH 14:
  batch 250 loss: 2.6651921520233155
  batch 500 loss: 2.6651778116226197
LOSS train 2.6651778116226197 valid 2.7437145709991455
PRECISION 0.10323857142857155 RECALL 0.1716 FSCORE 0.11225746031746023
EPOCH 15:
  batch 250 loss: 2.622401220321655
  batch 500 loss: 2.6358432550430297
LOSS train 2.6358432550430297 valid 2.7871150970458984
PRECISION 0.09288214285714307 RECALL 0.1588 FSCORE 0.09850584415584417
EPOCH 16:
  batch 250 loss: 2.6004431352615356
  batch 500 loss: 2.590961341381073
LOSS train 2.590961341381073 valid 2.67533016204834
PRECISION 0.1163680952380953 RECALL 0.1958 FSCORE 0.1288074314574316
EPOCH 17:
  batch 250 loss: 2.526645872116089
  batch 500 loss: 2.5178034129142763
LOSS train 2.5178034129142763 valid 2.792752742767334
PRECISION 0.11184428571428577 RECALL 0.1774 FSCORE 0.117369624819625
EPOCH 18:
  batch 250 loss: 2.4616027150154114
  batch 500 loss: 2.4189928975105284
LOSS train 2.4189928975105284 valid 2.6101937294006348
PRECISION 0.1443388095238095 RECALL 0.2088 FSCORE 0.15329698412698436
EPOCH 19:
  batch 250 loss: 2.382364200115204
  batch 500 loss: 2.3825896401405333
LOSS train 2.3825896401405333 valid 2.503265380859375
PRECISION 0.15978619047619058 RECALL 0.2244 FSCORE 0.1694725396825396
EPOCH 20:
  batch 250 loss: 2.307063593387604
  batch 500 loss: 2.2939239382743835
LOSS train 2.2939239382743835 valid 2.6593105792999268
PRECISION 0.14649619047619047 RECALL 0.2056 FSCORE 0.1538776190476193
EPOCH 21:
  batch 250 loss: 2.2647332806587217
  batch 500 loss: 2.2457148227691652
LOSS train 2.2457148227691652 valid 2.564349412918091
PRECISION 0.17166023809523845 RECALL 0.2306 FSCORE 0.17870666666666665
EPOCH 22:
  batch 250 loss: 2.1912441730499266
  batch 500 loss: 2.17361416721344
LOSS train 2.17361416721344 valid 2.808469533920288
PRECISION 0.1621199999999999 RECALL 0.2144 FSCORE 0.167636349206349
EPOCH 23:
  batch 250 loss: 2.1216954517364504
  batch 500 loss: 2.1183433122634887
LOSS train 2.1183433122634887 valid 3.179018974304199
PRECISION 0.14906119047619049 RECALL 0.2068 FSCORE 0.15261139971139984
EPOCH 24:
  batch 250 loss: 2.054180335521698
  batch 500 loss: 2.0583692569732666
LOSS train 2.0583692569732666 valid 2.748349666595459
PRECISION 0.1796419047619051 RECALL 0.2426 FSCORE 0.18674587301587314
EPOCH 25:
  batch 250 loss: 2.010658992767334
  batch 500 loss: 1.9890893597602843
LOSS train 1.9890893597602843 valid 3.5466201305389404
PRECISION 0.13739500000000002 RECALL 0.1972 FSCORE 0.14066187590187607
EPOCH 26:
  batch 250 loss: 1.933910430431366
  batch 500 loss: 1.9104588675498961
LOSS train 1.9104588675498961 valid 2.444998264312744
PRECISION 0.2256185714285716 RECALL 0.2942 FSCORE 0.23563380952380916
EPOCH 27:
  batch 250 loss: 1.8428819165229797
  batch 500 loss: 1.865483438014984
LOSS train 1.865483438014984 valid 3.0099711418151855
PRECISION 0.19608023809523828 RECALL 0.2592 FSCORE 0.20300793650793647
EPOCH 28:
  batch 250 loss: 1.7619965617656708
  batch 500 loss: 1.770037633895874
LOSS train 1.770037633895874 valid 3.025735378265381
PRECISION 0.20508000000000037 RECALL 0.2652 FSCORE 0.20943222222222235
EPOCH 29:
  batch 250 loss: 1.7329663424491881
  batch 500 loss: 1.75126043009758
LOSS train 1.75126043009758 valid 1.9279742240905762
PRECISION 0.3534185714285707 RECALL 0.403 FSCORE 0.3562404761904761
EPOCH 30:
  batch 250 loss: 1.6666919066905976
  batch 500 loss: 1.7033890085220338
LOSS train 1.7033890085220338 valid 2.073974370956421
PRECISION 0.35926047619047563 RECALL 0.4044 FSCORE 0.3576665079365081
EPOCH 31:
  batch 250 loss: 1.653617043018341
  batch 500 loss: 1.620478862285614
LOSS train 1.620478862285614 valid 2.075031042098999
PRECISION 0.36536428571428503 RECALL 0.41 FSCORE 0.36363587301587313
EPOCH 32:
  batch 250 loss: 1.5611214327812195
  batch 500 loss: 1.5131228902339935
LOSS train 1.5131228902339935 valid 1.9131110906600952
PRECISION 0.40843428571428464 RECALL 0.448 FSCORE 0.4049196825396828
EPOCH 33:
  batch 250 loss: 1.5259175024032592
  batch 500 loss: 1.5486207196712494
LOSS train 1.5486207196712494 valid 1.9565415382385254
PRECISION 0.39524999999999894 RECALL 0.4368 FSCORE 0.3946890476190475
EPOCH 34:
  batch 250 loss: 1.4994333846569061
  batch 500 loss: 1.4772500693798065
LOSS train 1.4772500693798065 valid 1.7757266759872437
PRECISION 0.46515666666666655 RECALL 0.489 FSCORE 0.45507333333333355
EPOCH 35:
  batch 250 loss: 1.3613302526473998
  batch 500 loss: 1.4199973838329316
LOSS train 1.4199973838329316 valid 1.6738545894622803
PRECISION 0.5006700000000002 RECALL 0.527 FSCORE 0.49352095238095245
EPOCH 36:
  batch 250 loss: 1.3721690126657486
  batch 500 loss: 1.3743682320117951
LOSS train 1.3743682320117951 valid 1.5853309631347656
PRECISION 0.5299966666666667 RECALL 0.55 FSCORE 0.5207052380952376
EPOCH 37:
  batch 250 loss: 1.3301579060554505
  batch 500 loss: 1.300057663679123
LOSS train 1.300057663679123 valid 1.5569788217544556
PRECISION 0.5445866666666667 RECALL 0.564 FSCORE 0.5366685714285717
EPOCH 38:
  batch 250 loss: 1.24537679541111
  batch 500 loss: 1.2545761651992797
LOSS train 1.2545761651992797 valid 1.65895676612854
PRECISION 0.5278033333333335 RECALL 0.539 FSCORE 0.5147439682539683
EPOCH 39:
  batch 250 loss: 1.2278754531145095
  batch 500 loss: 1.2320571867227554
LOSS train 1.2320571867227554 valid 1.6240752935409546
PRECISION 0.5410799999999998 RECALL 0.5488 FSCORE 0.5260290476190478
EPOCH 40:
  batch 250 loss: 1.1905735728740692
  batch 500 loss: 1.1946896365880966
LOSS train 1.1946896365880966 valid 1.5093222856521606
PRECISION 0.5752366666666673 RECALL 0.581 FSCORE 0.559313333333333
EPOCH 41:
  batch 250 loss: 1.166410594701767
  batch 500 loss: 1.1657922215461731
LOSS train 1.1657922215461731 valid 1.396180272102356
PRECISION 0.592916666666667 RECALL 0.6038 FSCORE 0.5801157142857143
EPOCH 42:
  batch 250 loss: 1.102502029776573
  batch 500 loss: 1.077406019926071
LOSS train 1.077406019926071 valid 1.3573691844940186
PRECISION 0.6193166666666672 RECALL 0.63 FSCORE 0.6072647619047612
EPOCH 43:
  batch 250 loss: 1.0683599762916565
  batch 500 loss: 1.092024829030037
LOSS train 1.092024829030037 valid 1.3809142112731934
PRECISION 0.6036100000000005 RECALL 0.6152 FSCORE 0.5928357142857144
EPOCH 44:
  batch 250 loss: 1.0401833463907242
  batch 500 loss: 1.0513821974396707
LOSS train 1.0513821974396707 valid 1.3279920816421509
PRECISION 0.6168500000000007 RECALL 0.625 FSCORE 0.6030457142857134
EPOCH 45:
  batch 250 loss: 1.0386998642086982
  batch 500 loss: 1.0304293477535247
LOSS train 1.0304293477535247 valid 1.4328056573867798
PRECISION 0.6008666666666674 RECALL 0.5976 FSCORE 0.5823661904761903
EPOCH 46:
  batch 250 loss: 0.9487051639556885
  batch 500 loss: 0.9703381637036801
LOSS train 0.9703381637036801 valid 1.3298641443252563
PRECISION 0.6346333333333339 RECALL 0.6286 FSCORE 0.6145342857142856
EPOCH 47:
  batch 250 loss: 0.9685642713308334
  batch 500 loss: 0.9642212863564491
LOSS train 0.9642212863564491 valid 1.3652653694152832
PRECISION 0.6406866666666674 RECALL 0.6296 FSCORE 0.6179380952380951
EPOCH 48:
  batch 250 loss: 0.9317342314720154
  batch 500 loss: 0.910438960313797
LOSS train 0.910438960313797 valid 1.2847261428833008
PRECISION 0.6527833333333339 RECALL 0.6492 FSCORE 0.6342771428571429
EPOCH 49:
  batch 250 loss: 0.8761038711667061
  batch 500 loss: 0.9168515493273734
LOSS train 0.9168515493273734 valid 1.33306884765625
PRECISION 0.6591933333333336 RECALL 0.6428 FSCORE 0.6329200000000001
EPOCH 50:
  batch 250 loss: 0.8682113490700721
  batch 500 loss: 0.8265725378394126
LOSS train 0.8265725378394126 valid 1.3207128047943115
PRECISION 0.655683333333334 RECALL 0.6456 FSCORE 0.6337257142857142
EPOCH 51:
  batch 250 loss: 0.7944490301012993
  batch 500 loss: 0.855243893802166
LOSS train 0.855243893802166 valid 1.3729151487350464
PRECISION 0.6548600000000004 RECALL 0.6444 FSCORE 0.6334095238095234
EPOCH 52:
  batch 250 loss: 0.7548137375116348
  batch 500 loss: 0.7887548156678676
LOSS train 0.7887548156678676 valid 1.2634822130203247
PRECISION 0.6874000000000006 RECALL 0.677 FSCORE 0.6665485714285704
EPOCH 53:
  batch 250 loss: 0.7784548397064209
  batch 500 loss: 0.7583656787872315
LOSS train 0.7583656787872315 valid 1.247926115989685
PRECISION 0.682116666666667 RECALL 0.6726 FSCORE 0.661035238095238
EPOCH 54:
  batch 250 loss: 0.7583854613900185
  batch 500 loss: 0.7699539805650711
LOSS train 0.7699539805650711 valid 1.2187944650650024
PRECISION 0.7019833333333341 RECALL 0.6842 FSCORE 0.6770999999999997
EPOCH 55:
  batch 250 loss: 0.766068165242672
  batch 500 loss: 0.730026362746954
LOSS train 0.730026362746954 valid 1.147755742073059
PRECISION 0.708050000000001 RECALL 0.697 FSCORE 0.6867980952380941
EPOCH 56:
  batch 250 loss: 0.7062833983302116
  batch 500 loss: 0.7211961929500103
LOSS train 0.7211961929500103 valid 1.2162758111953735
PRECISION 0.6840166666666676 RECALL 0.6724 FSCORE 0.6621447619047621
EPOCH 57:
  batch 250 loss: 0.6745175309777259
  batch 500 loss: 0.7548586381077766
LOSS train 0.7548586381077766 valid 1.1863240003585815
PRECISION 0.7011933333333338 RECALL 0.6898 FSCORE 0.6800614285714279
EPOCH 58:
  batch 250 loss: 0.6679040755331517
  batch 500 loss: 0.6802704664766789
LOSS train 0.6802704664766789 valid 1.2143595218658447
PRECISION 0.6908833333333337 RECALL 0.677 FSCORE 0.667670476190476
EPOCH 59:
  batch 250 loss: 0.6607844757735729
  batch 500 loss: 0.6254605171382427
LOSS train 0.6254605171382427 valid 1.2082715034484863
PRECISION 0.691533333333334 RECALL 0.6802 FSCORE 0.670107619047619
EPOCH 60:
  batch 250 loss: 0.6232103393375874
  batch 500 loss: 0.6209713823795319
LOSS train 0.6209713823795319 valid 1.2747544050216675
PRECISION 0.6908100000000006 RECALL 0.6724 FSCORE 0.6654442857142862
EPOCH 61:
  batch 250 loss: 0.6416262214183808
  batch 500 loss: 0.5830165817737579
LOSS train 0.5830165817737579 valid 1.2924739122390747
PRECISION 0.6807366666666674 RECALL 0.664 FSCORE 0.6566428571428569
EPOCH 62:
  batch 250 loss: 0.6083713608980179
  batch 500 loss: 0.619534382969141
LOSS train 0.619534382969141 valid 1.3908361196517944
PRECISION 0.6579666666666671 RECALL 0.6374 FSCORE 0.6303942857142854
EPOCH 63:
  batch 250 loss: 0.604215796649456
  batch 500 loss: 0.5485298944413662
LOSS train 0.5485298944413662 valid 1.198868989944458
PRECISION 0.6920666666666675 RECALL 0.6816 FSCORE 0.6707733333333334
EPOCH 64:
  batch 250 loss: 0.5478998575955629
  batch 500 loss: 0.5692206431776285
LOSS train 0.5692206431776285 valid 1.1855864524841309
PRECISION 0.7049433333333336 RECALL 0.6918 FSCORE 0.6824442857142855
EPOCH 65:
  batch 250 loss: 0.6005187427401543
  batch 500 loss: 0.5717102235555649
LOSS train 0.5717102235555649 valid 1.1618854999542236
PRECISION 0.7120666666666673 RECALL 0.6988 FSCORE 0.690059047619047
EPOCH 66:
  batch 250 loss: 0.5517215847074985
  batch 500 loss: 0.5713140310049057
LOSS train 0.5713140310049057 valid 1.3608205318450928
PRECISION 0.669243333333334 RECALL 0.6492 FSCORE 0.6421042857142856
EPOCH 67:
  batch 250 loss: 0.5767349437475204
  batch 500 loss: 0.556627522662282
LOSS train 0.556627522662282 valid 1.222698450088501
PRECISION 0.6939666666666672 RECALL 0.678 FSCORE 0.6690685714285716
EPOCH 68:
  batch 250 loss: 0.5233310308009386
  batch 500 loss: 0.5274019019007683
LOSS train 0.5274019019007683 valid 1.2547754049301147
PRECISION 0.690933333333334 RECALL 0.6746 FSCORE 0.6659228571428571
EPOCH 69:
  batch 250 loss: 0.49914064483344556
  batch 500 loss: 0.5307789602130651
LOSS train 0.5307789602130651 valid 1.1417886018753052
PRECISION 0.7075933333333343 RECALL 0.6996 FSCORE 0.6881538095238089
EPOCH 70:
  batch 250 loss: 0.503458038508892
  batch 500 loss: 0.5107295139729977
LOSS train 0.5107295139729977 valid 1.2351710796356201
PRECISION 0.6970100000000009 RECALL 0.6804 FSCORE 0.672915238095238
EPOCH 71:
  batch 250 loss: 0.48762373530864717
  batch 500 loss: 0.491446000918746
LOSS train 0.491446000918746 valid 1.3259265422821045
PRECISION 0.6779833333333338 RECALL 0.6618 FSCORE 0.6534923809523807
EPOCH 72:
  batch 250 loss: 0.44610483464598655
  batch 500 loss: 0.47560467474162577
LOSS train 0.47560467474162577 valid 1.2304505109786987
PRECISION 0.7104266666666674 RECALL 0.6952 FSCORE 0.6868833333333328
EPOCH 73:
  batch 250 loss: 0.46494052186608315
  batch 500 loss: 0.44707133461534976
LOSS train 0.44707133461534976 valid 1.1449384689331055
PRECISION 0.7244333333333343 RECALL 0.7058 FSCORE 0.7000923809523812
EPOCH 74:
  batch 250 loss: 0.43586784102022647
  batch 500 loss: 0.43879959726333617
LOSS train 0.43879959726333617 valid 1.2198048830032349
PRECISION 0.7055766666666671 RECALL 0.6886 FSCORE 0.6808585714285711
EPOCH 75:
  batch 250 loss: 0.42689033307135105
  batch 500 loss: 0.42274354718625545
LOSS train 0.42274354718625545 valid 1.1602461338043213
PRECISION 0.7124500000000006 RECALL 0.7014 FSCORE 0.6914428571428566
EPOCH 76:
  batch 250 loss: 0.43817291025817395
  batch 500 loss: 0.39142508879303933
 LOSS train 0.39142508879303933 valid 1.3133506774902344
PRECISION 0.6939500000000007 RECALL 0.6782 FSCORE 0.6701761904761905
EPOCH 77:
  batch 250 loss: 0.39337358333170414
  batch 500 loss: 0.36906083332002165
LOSS train 0.36906083332002165 valid 1.2753691673278809
PRECISION 0.7100266666666674 RECALL 0.6856 FSCORE 0.6811242857142853
