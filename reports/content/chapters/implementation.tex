\section{Train/Test Data Extraction}

\subsection{Ground Truth Glyph Examples}

The ground truth data provided by the contest contained image files, each with multiple bounding boxes and glyphs. To extract this data for the purposes of training and evaluation of classification and bounding methods, each original image was copied and cropped to contain only one glyph, generating a single cropped image, cut to the exact bounding box given, for each glyph \seefig{cropped_ground_truth_glyphs}. To extract the isolated glyph images for training, the following method is used. For each image in the training data, the image was loaded, and the COCO file provided by the contest was utilized to find each annotated glyph. For each glyph, the loaded image was duplicated and cropped to the bounding box provided in the COCO file. The cropped image was then saved to file either separated by glyph class, or without any class label.
Each cropped glyph file was saved with information on which image the glyph was cropped from, the style it was written in \seefig{xi_footmarktype}, and the preservation of the glyph \seefig{omega_basetype}.

These methods were used to extract glyph images from both the binarized images and un-altered images. Similar processes were also used to generate training data for the various CNNs, and external tools utilized in the project.

\begin{figure}
    \caption{Ground Truth Glyph Examples}
    \label{fig:cropped_ground_truth_glyphs}
    \begin{center}
    \includegraphics[width=0.75\textwidth]{{glyphs/cropped_ground_truth_glyphs.png}}
    \end{center}
    Four examples of the bounded glyphs provided by the contest training data. All the glyphs provided in the training and test data are uppercase and bounded by hand-drawn bounding boxes, which appear here. The first glyph, a $\Gamma$, is well formed, but has the noise from another glyph on the left side and is truncated by the bounding box on the right side. Similarly, the second glyph, an $A$, is well formed, but has the noise from another glyph on the left side and is truncated by the bounding box on the right and bottom sides. The third glyph, an $I$, is not cut off, but does have a line from a neighboring glyph as well. It also shows the edge of the papyrus. The fourth glyph, another $A$, is an example of the varying image quality of the training data, showing obvious artifacting or distortion.
\end{figure}

\begin{figure}
    \caption{Styles of $\Xi$ in the Training Data}
    \label{fig:xi_footmarktype}
    \begin{center}
    \includegraphics[width=0.75\textwidth]{{glyphs/xi_footmarktype.png}}
    \end{center}
    Examples of the six unique styles for the letter $\Xi$, as determined by the authors of the contest this project was based on. Numbered from 1-6 left to right. The furthest right image of a $\Xi$ was resized for the purpose of display in the figure, which has increased the blurriness of the image.
\end{figure}

\begin{figure}
    \caption{Examples of Preservation Levels in $\Omega$'s in Training Data}
    \label{fig:omega_basetype}
    \begin{center}
    \includegraphics[width=0.75\textwidth]{{glyphs/omega_basetype.png}}
    \end{center}
    Examples of the three levels of preservation for the letter $\Omega$ (appearing as the unical manuscript form, which looks similar to a larger lowercase modern $\omega$), as determined by the authors of the contest this project was based on. Numbered from 1-3 left to right. There are three examples of each level to display three examples of the various degradations that lead to being in each level. The least degraded level (1) is defined as a complete or nearly complete glyph. The second least degraded level (2) is defined as an incomplete letter that unambiguously belongs to one class. The most degraded level in the dataset (3), requires context to determine its actual class, as the glyph may appear to potentially be one of multiple classes when viewed in isolation.
\end{figure}

\subsection{Tightly Bound Glyph Examples}
\todo{TODO}

\subsection{Greek Language Corpus}

To form the training data for a language model, the entirety of the Greek section of the Perseus Digital Library \cite{Perseus} dataset was downloaded. The data was converted from XML to plain text and cleaned of all non-standard Greek letters utilizing a mix of regex, python string functions and a list of the glyphs that appear in the training set of images. When removing invalid characters, the passages that they appeared in were also removed to ensure the integrity of the training data. To ensure the language data was as close to the image training data as possible, all diacritics and punctuation marks were removed, and the letters were converted to their uppercase variant. The remaining data contains $~46,400,000$ glyphs. This data was used to generate five smaller files for the purposes of training language models by randomly selecting chunks from the data.


\section{Binarization}
\todo{NEW}

A perfect binarized version of a papyrus image is defined by a background color and ink color, where the ink is black, and the background is white. Each pixel in an image is altered so that it is either black or white, depending on if it is ink or anything else (background). As no ground truth for ink was given to train on, all the methods for binarization are based on either pre-trained CNNs or algorithmic solutions utilized in similar problems.

\subsection{Clustering}
\todo{NEW, TODO: ADD FIGURE}

Clustering based binarization is performed with k-means clustering. For each image, the pixels of the image were placed in a 3d space with their red, green, and blue values being used to determine their location. Pixels were then clustered in groups using k-means clustering. The pixel cluster which contained the value closest to black was then utilized as the ink cluster, with all pixels not in that cluster being labeled as the background.

\subsection{Filtering}
\todo{NEW, TODO: ADD FIGURE}

Filtering based binarizations are generated utilizing the Gabor filtering  \cite{Gabor1949, Gabor1948} to find clusters of pixels that are not like those around them, which could be glyphs. Gabor filtering is based on a linear filter, but when multiple linear filters are grouped together with a set rotation applied to each subsequent filter, a 2d filter can be generated to generate a simplification of Gabor wavelets \cite{Gabor1949, Gabor1948}. Using these 2d filters, this method of binarization seeks to find features in an image that has a specific frequency, which in the case of glyphs might be the width or height of a single line in a glyph.

This method requires an assumption on the wave frequency that may generate the glyphs, so a manual heuristic method was utilized to generate the function that most accurately generated the correct wave for the images in the training dataset.

To utilize the approximation on a Gabor wavelet on the input, the input is read in and converted to a greyscale image before having the collection of Gabor filters applied. This reduces the channels from three to one and allows the Gabor wavelet approximation to work on the image. The outputs of each filter in the wavelet approximation are then averaged together to generate a new greyscale image, which can then be binarized using naive thresholding.

\subsection{Input Invarient Thresholding}
\todo{NEW, TODO: ADD FIGURE}

In a similar fashion to utilizing the Gabor filter or Gabor wavelet, thresholding utilizing an input invariant method, such as the one proposed by \cite{Bar-Yosef2005, Bar-Yosef2007} requires consistency in the input or a strong method for determining the correct threshold. Using clustering, as discussed above, could be a potential method to find a threshold, however the method them becomes a variant of clustering, so this method was not implemented past the initial stage of picking an arbitrarily chosen threshold value and binarizing the input by converting it greyscale and then binarizing the greyscale image by converting all values darker than the threshold to black and all other values to white.

\subsection{DP-Linknet}
\todo{NEW, TODO: ADD FIGURE}

Although it was out of the scope of this project to train a CNN to binarize an image, Convolutional Neural Networks have already been generated and trained on the problem of binarization of historical texts. DP-Linknet \cite{Xiong} is one such CNN, trained on historical texts and implemented in Python 3 utilizing common deep learning tools such as PyTorch \cite{PyTorch} and Scikit-Learn \cite{Scikit}. As the Python source code for DP-Linknet is publicly available on \href{https://github.com/beargolden/DP-LinkNet}{GitHub}, it was a simple task to implement it into the project and add a simple API that integrates the CNN into the pipeline.

\textit{Note: All code contained in the} \code{src/binarization/dp\_linknet} \textit{folder is either directly from the GitHub reposity or highly based on that code. I take no credit for this code, as extraordinarily little of my own work was done to integrate the work of Xiong et al. into this project.}

\subsection{BiNet}
\todo{TODO}


\section{Glyph Bounding}

\subsection{Bounding Boxes}
\todo{NEW}

Bounding boxes are represented internally by an object that holds the PASCAL Visual Object Classes (VOC) representation of a bounding box \seefig{pascalbbox}. Using this method of representation, compared to a tuple or list, allows for operations on bounding boxes to be standardized and for bounding boxes to be represented in a way that is abstracted from their format. Allowing this abstract bounding box format assists with the problem of having multiple constraints on how bounding boxes are utilized in the pipeline, such as importing and exporting COCO formatted bounding boxes \seefig{cocobbox}, calculating metrics based on PASCAL VOC formatted bounding boxes \seefig{pascalbbox}, and utilizing tools and methods that require YOLO formatted bounding boxes \seefig{yolobbox}. Bounding box objects are then able to utilize internal methods for operations such as finding the IOU (intersection over union) \seefig{IOU} between two bounding boxes, one dimensional IOU \seefig{1dIOU}, and generating cropped images from lists of bounding boxes.

\begin{figure}
    \caption{PASCAL VOC Bounding Box Examples}
    \label{fig:pascalbbox}
    \begin{center}
    \includegraphics[width=0.75\textwidth]{{bounding/bbox_examples_pascal.png}}
    \end{center}
    Three examples of bounding boxes, on a $N$, $\Upsilon$, and a $\Xi$ (left to right). For these examples, the red lines denote the pixels that are inside of the bounding boxes. In the PASCAL VOC bounding box format, bounding boxes are described using the top left corner (marked in green) of the box in absolute pixels, and the bottom right corner of the box (marked in blue) in absolute pixels. In this example, the boxes shown above would be denoted in the PASCAL VOC format as follows: $N:(6, 13, 82, 97)$, $\Upsilon:(1, 3, 89, 103)$, $\Xi:(6, 16, 126, 96)$.
\end{figure}

\begin{figure}
    \caption{COCO Format Bounding Box Examples}
    \label{fig:cocobbox}
    \begin{center}
    \includegraphics[width=0.75\textwidth]{{bounding/bbox_examples_coco.png}}
    \end{center}
    Three examples of bounding boxes, on a $N$, $\Upsilon$, and a $\Xi$ (left to right). In the COCO bounding box format, bounding boxes are described using the top left corner (marked in green) of the box  in absolute pixels and the box's width (purple) and height (blue) in absolute pixels. In this example, the boxes shown above would be denoted in the COCO format as follows: $N:(6, 13, 76, 84)$, $\Upsilon:(1, 3, 88, 100)$, $\Xi:(6, 16, 120, 80)$.
\end{figure}

\begin{figure}
    \caption{YOLO Format Bounding Box Examples}
    \label{fig:yolobbox}
    \begin{center}
    \includegraphics[width=0.75\textwidth]{{bounding/bbox_examples_yolo.png}}
    \end{center}
    Three examples of bounding boxes, on a $N$, $\Upsilon$, and a $\Xi$ (left to right). In the YOLO bounding box format, bounding boxes are described using the center of the box (marked in green) in relative coordinates (based on image width and height) and the box's width (purple) and height (blue) also in relative coordinates. In this example, the boxes shown above would be denoted in the YOLO format as follows (rounded to three decimal places): $N:(0.319, 0.519, , 0.532, 0.739)$, $\Upsilon:(0.469, 0.469, 0.917, 0.885)$, $\Xi:(0.431, 0.487, 0.784, 0.696)$.
\end{figure}

\begin{figure}
    \caption{2D Intersection Over Union (IOU) Examples}
    \label{fig:IOU}
    \begin{center}
      \begin{subfigure}[b]{0.45\textwidth}
           \centering
           \caption{Bounding Boxes With A Low IOU Value}
           \label{fig:lowIOU}
           \includegraphics[width=\textwidth]{{bounding/lowIOU.jpg}}
       \end{subfigure}
       \hfill
       \begin{subfigure}[b]{0.45\textwidth}
           \centering
           \caption{Bounding Boxes With A High IOU Value}
           \label{fig:highIOU}
           \includegraphics[width=\textwidth]{{bounding/highIOU.jpg}}
       \end{subfigure}
    \end{center}
    The IOU of two bounding boxes is a measure of how similar they are (with a range denoted by the interval $[0,1]$). It is calculated using the following formula, where $a$ and $b$ are bounding boxes represented by sets of pixels.
    \begin{equation}
      IOU(a,b) = \frac{a \cap b}{a \cup b}
    \end{equation}
    Figure \ref{fig:lowIOU} is an example of two bounding boxes with a low IOU value as the union of the sets (green and blue regions) is much larger than the intersection of the sets (blue region). Figure \ref{fig:highIOU} is an example of two bounding boxes with a high IOU value as the union of the sets (green and blue regions) is similar to the intersection of the sets (blue region).
\end{figure}

\begin{figure}
    \caption{1D Intersection Over Union (IOU) Examples}
    \label{fig:1dIOU}
    \begin{center}
      \begin{subfigure}[b]{0.45\textwidth}
           \centering
           \caption{1D Horizontal IOU Calculation}
           \label{fig:horizonalIOU}
           \includegraphics[width=\textwidth]{{bounding/horizontalIOU.jpg}}
       \end{subfigure}
       \hfill
       \begin{subfigure}[b]{0.45\textwidth}
           \centering
           \caption{1D Vertical IOU Calculation}
           \label{fig:verticalIOU}
           \includegraphics[width=\textwidth]{{bounding/verticalIOU.jpg}}
       \end{subfigure}
    \end{center}
    To find the 1D IOUs of a pair of bounding boxes the boxes each treated as one dimensional objects and the IOU is found as normal, calculating the intersection and union for the $x$ and $y$ dimensions separately. In Figure \ref{fig:horizonalIOU} the horizonal IOU is low, with the intersection of the sets in 1D (blue region) being divided by the union of the sets (green and blue regions). Figure \ref{fig:verticalIOU} has a high vertical IOU, with the intersection of the sets (blue region) being divided by the union of the sets (green and blue regions).
\end{figure}

\subsection{Connected Components}
\todo{TODO}

\subsection{Sliding Window Region Based Convolution Neural Networks}
\todo{TODO}

\subsection{Binarization Assisted CNN}
\todo{TODO}

\section{Line Bounding}

\subsection{Point Of Interest Clustering}
\todo{DOG + POI + adaptive dbscan}

\subsection{Point Of Interest Clustering}
\todo{TODO}

\subsection{Adjacent Glyph Clustering}
\todo{TODO}

\subsection{Overlaping Glyph Clustering}
\todo{TODO}


\section{Classification}

\subsection{Transfer Learning}
\todo{TODO}

\subsection{Convolutional Neural Networks}
\todo{TODO}

\subsection{Stochastic Language Models}
\todo{TODO}

\subsection{Neural Netork Language Models}
\todo{TODO}


\section{Existing Pipelines}

For the sake of completeness, Tesseract \cite{SmithTesseract}, a popular OCR engine sponsored by Google \cite{Vincent} and Ocular, another OCR engine based on Stochastic models were integrated into the pipeline using pytesseract \cite{Lee} and by calling jar files from python respectively.
