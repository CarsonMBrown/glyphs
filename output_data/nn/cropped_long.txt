"C:\Users\Carson Brown\.conda\envs\glyphs\python.exe" "C:/Users/Carson Brown/git/glyphs/src/main.py"
Using cache found in C:\Users\Carson Brown/.cache\torch\hub\pytorch_vision_v0.10.0
Starting...
Image input directory: dataset\glyphs\eval\generated\cropped
transform Compose(
    Resize(size=224, interpolation=bilinear, max_size=None, antialias=None)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Input Vector Size: None
C:\Users\Carson Brown\.conda\envs\glyphs\lib\site-packages\torchvision\models\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.
  warnings.warn(
Image input directory: dataset\glyphs\train\generated\cropped
transform Compose(
    Resize(size=224, interpolation=bilinear, max_size=None, antialias=None)
    RandomPerspective(p=0.5)
    ColorJitter(brightness=[0.7, 1.3], contrast=[0.7, 1.3], saturation=[0.7, 1.3], hue=[-0.1, 0.1])
    RandomAffine(degrees=[-5.0, 5.0], shear=[-5.0, 5.0])
    RandomCrop(size=(224, 224), padding=None)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Training set has 5000 instances
EPOCH 0:
  batch 250 loss: 2.752447624206543
LOSS train 2.752447624206543 valid 2.5428383350372314
PRECISION 0.11426016067266061 RECALL 0.246 FSCORE 0.14503200133200128
EPOCH 1:
  batch 250 loss: 2.4218219685554505
LOSS train 2.4218219685554505 valid 2.1165060997009277
PRECISION 0.3694839285714287 RECALL 0.407 FSCORE 0.3579904761904762
EPOCH 2:
  batch 250 loss: 2.151377185344696
LOSS train 2.151377185344696 valid 1.9380916357040405
PRECISION 0.4399452380952382 RECALL 0.4305 FSCORE 0.4030009379509381
EPOCH 3:
  batch 250 loss: 1.9107631936073304
LOSS train 1.9107631936073304 valid 1.7065274715423584
PRECISION 0.5474357142857141 RECALL 0.5235 FSCORE 0.5035911255411254
EPOCH 4:
  batch 250 loss: 1.6910719299316406
LOSS train 1.6910719299316406 valid 1.869960069656372
PRECISION 0.5296732142857145 RECALL 0.525 FSCORE 0.4917818903318905
EPOCH 5:
  batch 250 loss: 1.6089026679992675
LOSS train 1.6089026679992675 valid 1.3399397134780884
PRECISION 0.6393452380952384 RECALL 0.6045 FSCORE 0.5921761904761902
EPOCH 6:
  batch 250 loss: 1.4682944235801696
LOSS train 1.4682944235801696 valid 1.4170317649841309
PRECISION 0.5953291666666669 RECALL 0.5875 FSCORE 0.5611608835608836
EPOCH 7:
  batch 250 loss: 1.432128768324852
LOSS train 1.432128768324852 valid 1.3576616048812866
PRECISION 0.6405166666666666 RECALL 0.613 FSCORE 0.592622546897547
EPOCH 8:
  batch 250 loss: 1.361819882631302
LOSS train 1.361819882631302 valid 1.363681674003601
PRECISION 0.6789738095238097 RECALL 0.6185 FSCORE 0.6162924963924964
EPOCH 9:
  batch 250 loss: 1.800698980808258
  batch 500 loss: 1.5854177589416505
  batch 750 loss: 1.5499614191055298
  batch 1000 loss: 1.5188696262836456
LOSS train 1.5188696262836456 valid 1.41829514503479
PRECISION 0.5998484215589479 RECALL 0.5931020733652314 FSCORE 0.565814120090436
EPOCH 10:
  batch 250 loss: 1.894447163105011
  batch 500 loss: 1.8609310092926026
  batch 750 loss: 1.8132833065986633
  batch 1000 loss: 1.7866373376846314
LOSS train 1.7866373376846314 valid 1.5381724834442139
PRECISION 0.5923644338118021 RECALL 0.5815390749601275 FSCORE 0.5535379946564156
EPOCH 11:
  batch 250 loss: 1.8163833985328675
  batch 500 loss: 1.8293896737098694
  batch 750 loss: 1.754228785276413
  batch 1000 loss: 1.7188602516651152
LOSS train 1.7188602516651152 valid 1.4981776475906372
PRECISION 0.5471502663338789 RECALL 0.5572169059011166 FSCORE 0.5141088355203185
EPOCH 12:
  batch 250 loss: 1.7074262175559998
  batch 500 loss: 1.7434968400001525
  batch 750 loss: 1.7021087293624877
  batch 1000 loss: 1.6712462849617005
LOSS train 1.6712462849617005 valid 1.6621488332748413
PRECISION 0.5359248816485658 RECALL 0.5289074960127595 FSCORE 0.49987506733918213
EPOCH 13:
  batch 250 loss: 1.7353227224349976
  batch 500 loss: 1.7075550980567933
  batch 750 loss: 1.6786716513633728
  batch 1000 loss: 1.6798284955024718
LOSS train 1.6798284955024718 valid 1.7654753923416138
PRECISION 0.489106842231842 RECALL 0.5023923444976075 FSCORE 0.45963529364616446
EPOCH 14:
  batch 250 loss: 1.7372601115703583
  batch 500 loss: 1.6840159063339233
  batch 750 loss: 1.7100342338085175
  batch 1000 loss: 1.6799348723888397
LOSS train 1.6799348723888397 valid 1.4140466451644897
PRECISION 0.5875573561428823 RECALL 0.5925039872408294 FSCORE 0.5565906625715237
EPOCH 15:
  batch 250 loss: 1.7270746009349822
  batch 500 loss: 1.7341500713825226
  batch 750 loss: 1.6837991309165954
  batch 1000 loss: 1.6706844909191132
LOSS train 1.6706844909191132 valid 1.6358150243759155
PRECISION 0.5776584770005825 RECALL 0.5426634768740035 FSCORE 0.524599349569445
EPOCH 16:
  batch 250 loss: 1.7143896067142486
  batch 500 loss: 1.678927003145218
  batch 750 loss: 1.6768455770015716
  batch 1000 loss: 1.6538728668689728
LOSS train 1.6538728668689728 valid 1.5358339548110962
PRECISION 0.5657854245922427 RECALL 0.5637958532695373 FSCORE 0.5300399018401414
EPOCH 17:
  batch 250 loss: 1.661782763004303
  batch 500 loss: 1.6993749463558196
  batch 750 loss: 1.6165450282096863
  batch 1000 loss: 1.6535788238048554
LOSS train 1.6535788238048554 valid 1.5305566787719727
PRECISION 0.5713195552011342 RECALL 0.5729665071770332 FSCORE 0.5339433205461913
